{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ACG_Text_Features.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "VzH8cGgXDOOz",
        "colab_type": "code",
        "outputId": "00473c0d-344d-4b28-8808-0f2852e5f612",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        }
      },
      "source": [
        "!unzip '/content/drive/My Drive/Automated Caption Generator/Flickr8k_text.zip'"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  /content/drive/My Drive/Automated Caption Generator/Flickr8k_text.zip\n",
            "  inflating: CrowdFlowerAnnotations.txt  \n",
            "  inflating: ExpertAnnotations.txt   \n",
            "  inflating: Flickr8k.lemma.token.txt  \n",
            "   creating: __MACOSX/\n",
            "  inflating: __MACOSX/._Flickr8k.lemma.token.txt  \n",
            "  inflating: Flickr8k.token.txt      \n",
            "  inflating: Flickr_8k.devImages.txt  \n",
            "  inflating: Flickr_8k.testImages.txt  \n",
            "  inflating: Flickr_8k.trainImages.txt  \n",
            "  inflating: readme.txt              \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "15btB3ScDRlw",
        "colab_type": "code",
        "outputId": "51bfc1ed-7449-48ee-c58f-95900c1533f5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "#importing necessary packages\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import keras\n",
        "import tensorflow.keras as ks\n",
        "import tensorflow.keras.preprocessing.image as image;from keras.preprocessing.text import Tokenizer\n",
        "from keras.applications.vgg16 import VGG16\n",
        "from keras.models import Model\n",
        "import os\n",
        "from PIL import Image\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "from nltk.corpus import stopwords\n",
        "from tqdm import tqdm\n",
        "from nltk.tokenize import word_tokenize "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HOtxApzLDvmf",
        "colab_type": "code",
        "outputId": "81719d8c-35b6-426e-e1ed-de78aad62d45",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "stop_words = set(stopwords.words())\n",
        "\n",
        "#creating text file without ids i.e. \"text\"\n",
        "f = open('/content/Flickr8k.token.txt',\"r\")\n",
        "filtered_text,text,docID,final_text = [],[],[],[]\n",
        "j = 0\n",
        "\n",
        "#just taking 7000 out of text as what is we want for training data\n",
        "for i in f:\n",
        "#  print(i)\n",
        "  ID = i.split()[0]\n",
        "  t = i.split()[1:] \n",
        "  text.append(t)\n",
        "  docID.append(str(ID))\n",
        "\n",
        "#from array containing ID and text extracting just text in text list \n",
        "# removing words having length less than 1 and also converting them to lowercase\n",
        "for i in text:\n",
        "  filtered_text.append([y.lower() for y in i if len(y)>1])\n",
        "\n",
        "#removing punctuations, again all the filtered data is filled in text variable as we don't need the data(previously stored) in it anymore \n",
        "text = []\n",
        "for i in filtered_text:\n",
        "  text.append([z for z in i if z.isalpha()])\n",
        "\n",
        "#joining the words as lines in filtered_text variable as we don't need the data(previously stored) in it anymore \n",
        "filtered_text = ['startseq '+\" \".join(x)+' endseq' for x in text]\n",
        "#print(filtered_text)\n",
        "#converting the text into numbers\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(filtered_text)\n",
        "\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "#print(vocab_size)\n",
        "#word_num is array containing the numers in form of numbers\n",
        "word_num = tokenizer.texts_to_sequences(filtered_text)\n",
        "print(word_num[0:20])\n",
        "#finally the processed text data is prepared i.e. final_text\n",
        "for i,j in zip(docID,word_num):\n",
        "  final_text.append([i,j])\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1, 41, 3, 89, 168, 6, 118, 52, 393, 11, 390, 3, 27, 5075, 690, 2], [1, 18, 311, 63, 192, 116, 2], [1, 39, 18, 118, 63, 192, 2402, 2], [1, 39, 18, 118, 4, 390, 19, 59, 2402, 2], [1, 39, 18, 3, 89, 168, 311, 63, 192, 2949, 2], [1, 14, 8, 7, 847, 8, 16, 340, 2], [1, 14, 8, 7, 8, 33, 9, 136, 81, 5, 4, 153, 2], [1, 14, 8, 7, 13, 8, 9, 26, 991, 16, 634, 21, 136, 81, 3, 4, 71, 2], [1, 12, 30, 11, 734, 2613, 87, 21, 136, 81, 5, 4, 153, 2], [1, 12, 30, 5, 716, 792, 316, 136, 81, 2], [1, 39, 18, 184, 3, 604, 105, 3, 46, 11, 593, 1176, 9, 59, 217, 3, 963, 2], [1, 39, 18, 6, 48, 3, 46, 11, 54, 593, 1176, 2], [1, 51, 18, 3, 4, 40, 113, 9, 3376, 3, 46, 11, 13, 2403, 9, 1176, 5, 147, 2], [1, 185, 6, 18, 9, 1547, 48, 3, 46, 11, 1176, 762, 2], [1, 25, 18, 9, 1547, 762, 80, 3, 4, 40, 2], [1, 10, 641, 5, 159, 34, 28, 8, 105, 53, 146, 2], [1, 10, 641, 5, 4, 159, 19, 681, 13, 8, 6, 1246, 1477, 2], [1, 10, 612, 5, 159, 80, 9, 13, 7, 14, 8, 48, 84, 19, 146, 2], [1, 405, 10, 763, 5, 117, 159, 9, 28, 8, 2], [1, 10, 262, 5, 159, 49, 345, 11, 8, 48, 5, 167, 2]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wcbXomUp6wrd",
        "colab_type": "code",
        "outputId": "ad9f1ff7-934d-4ab7-a0b1-b1d7024f5cf0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "\n",
        "import os\n",
        "%cd drive\n",
        "%cd My Drive/Automated Caption Generator/\n",
        "print(os.getcwd())\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive\n",
            "/content/drive/My Drive/Automated Caption Generator\n",
            "/content/drive/My Drive/Automated Caption Generator\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g051pbQuHhB7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "import pickle\n",
        "\n",
        "f = open('text.pickle','wb')\n",
        "pickle.dump(final_text,f)\n",
        "f.close()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZQuDGDq5GhFs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}